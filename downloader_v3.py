#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æŠ–éŸ³è§†é¢‘ä¸‹è½½å™¨ V3 - é›†æˆè§£ææœåŠ¡ç‰ˆæœ¬
ä½¿ç”¨ç‹¬ç«‹çš„è§£ææœåŠ¡è·å–è§†é¢‘ä¿¡æ¯ï¼Œæé«˜ç¨³å®šæ€§å’ŒæˆåŠŸç‡
"""

import os
import re
import json
import time
import requests
import asyncio
import aiohttp
from typing import Dict, List, Optional, Union
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse, parse_qs
import logging
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DouYinDownloaderV3:
    """æŠ–éŸ³ä¸‹è½½å™¨V3 - ä½¿ç”¨è§£ææœåŠ¡"""

    def __init__(self, parsing_service_url: str = "http://localhost:5000"):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            parsing_service_url: è§£ææœåŠ¡åœ°å€
        """
        self.parsing_service_url = parsing_service_url
        self.session = requests.Session()
        self.download_dir = Path("downloads")
        self.download_dir.mkdir(exist_ok=True)

        # é…ç½®
        self.config = {
            'max_retries': 3,
            'timeout': 30,
            'chunk_size': 8192,
            'max_workers': 5,
            'use_proxy': False,
            'force_refresh': False
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }

        # æ£€æŸ¥è§£ææœåŠ¡
        self._check_parsing_service()

    def _check_parsing_service(self):
        """æ£€æŸ¥è§£ææœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(f"{self.parsing_service_url}/health", timeout=5)
            if response.status_code == 200:
                logger.info(f"âœ… è§£ææœåŠ¡æ­£å¸¸: {self.parsing_service_url}")
            else:
                logger.warning(f"âš ï¸ è§£ææœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°è§£ææœåŠ¡: {e}")
            logger.error("è¯·å…ˆå¯åŠ¨è§£ææœåŠ¡: cd parsing_service && python app.py")
            logger.error("æˆ–ä½¿ç”¨Docker: docker-compose up")
            raise

    def parse_video(self, url: str, cookies: Dict = None) -> Optional[Dict]:
        """
        è°ƒç”¨è§£ææœåŠ¡è·å–è§†é¢‘ä¿¡æ¯

        Args:
            url: è§†é¢‘URL
            cookies: Cookieå­—å…¸

        Returns:
            è§†é¢‘ä¿¡æ¯å­—å…¸
        """
        data = {
            'url': url,
            'use_proxy': self.config['use_proxy'],
            'force_refresh': self.config['force_refresh']
        }

        if cookies:
            data['cookies'] = cookies

        try:
            response = self.session.post(
                f"{self.parsing_service_url}/parse",
                json=data,
                timeout=self.config['timeout']
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    return result.get('data')
                else:
                    logger.error(f"è§£æå¤±è´¥: {result.get('error')}")
            else:
                logger.error(f"è§£ææœåŠ¡é”™è¯¯: {response.status_code}")

        except Exception as e:
            logger.error(f"è°ƒç”¨è§£ææœåŠ¡å¤±è´¥: {e}")

        return None

    def batch_parse(self, urls: List[str], cookies: Dict = None) -> List[Dict]:
        """
        æ‰¹é‡è§£æè§†é¢‘

        Args:
            urls: URLåˆ—è¡¨
            cookies: Cookieå­—å…¸

        Returns:
            è§£æç»“æœåˆ—è¡¨
        """
        data = {
            'urls': urls,
            'use_proxy': self.config['use_proxy']
        }

        if cookies:
            data['cookies'] = cookies

        try:
            response = self.session.post(
                f"{self.parsing_service_url}/batch_parse",
                json=data,
                timeout=120
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    return result.get('results', [])

        except Exception as e:
            logger.error(f"æ‰¹é‡è§£æå¤±è´¥: {e}")

        return []

    def download_video(self, video_info: Dict, save_dir: Path = None) -> bool:
        """
        ä¸‹è½½è§†é¢‘æ–‡ä»¶

        Args:
            video_info: è§†é¢‘ä¿¡æ¯
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not video_info:
            return False

        save_dir = save_dir or self.download_dir
        save_dir.mkdir(exist_ok=True)

        # æ„å»ºæ–‡ä»¶å
        video_id = video_info.get('video_id', 'unknown')
        title = self._sanitize_filename(video_info.get('title', video_id))
        author = self._sanitize_filename(video_info.get('author', 'unknown'))

        # å¤„ç†å›¾æ–‡ä½œå“
        if video_info.get('is_image'):
            return self._download_images(video_info, save_dir, title, author)

        # ä¸‹è½½è§†é¢‘
        video_url = video_info.get('video_url')
        if not video_url:
            logger.error(f"æ²¡æœ‰æ‰¾åˆ°è§†é¢‘URL: {video_id}")
            return False

        filename = f"{author}_{title}_{video_id}.mp4"
        filepath = save_dir / filename

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
        if filepath.exists():
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            self.stats['skipped'] += 1
            return True

        try:
            logger.info(f"å¼€å§‹ä¸‹è½½: {filename}")

            # ä¸‹è½½è§†é¢‘
            response = self.session.get(video_url, stream=True, timeout=30)
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))

            with open(filepath, 'wb') as f:
                with tqdm(total=total_size, unit='B', unit_scale=True, desc=filename[:30]) as pbar:
                    for chunk in response.iter_content(chunk_size=self.config['chunk_size']):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))

            logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {filename}")
            self.stats['success'] += 1

            # ä¸‹è½½å°é¢ï¼ˆå¯é€‰ï¼‰
            cover_url = video_info.get('cover_url')
            if cover_url:
                self._download_cover(cover_url, save_dir, f"{author}_{title}_{video_id}.jpg")

            return True

        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {filename} - {e}")
            self.stats['failed'] += 1

            # æ¸…ç†æœªå®Œæˆçš„æ–‡ä»¶
            if filepath.exists():
                filepath.unlink()

            return False

    def _download_images(self, video_info: Dict, save_dir: Path, title: str, author: str) -> bool:
        """ä¸‹è½½å›¾æ–‡ä½œå“"""
        images = video_info.get('images', [])
        if not images:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
            return False

        video_id = video_info.get('video_id', 'unknown')
        image_dir = save_dir / f"{author}_{title}_{video_id}"
        image_dir.mkdir(exist_ok=True)

        logger.info(f"ä¸‹è½½å›¾æ–‡ä½œå“: {len(images)} å¼ å›¾ç‰‡")

        success_count = 0
        for i, image_url in enumerate(images, 1):
            try:
                response = self.session.get(image_url, timeout=30)
                response.raise_for_status()

                image_path = image_dir / f"image_{i:02d}.jpg"
                with open(image_path, 'wb') as f:
                    f.write(response.content)

                success_count += 1
                logger.debug(f"ä¸‹è½½å›¾ç‰‡ {i}/{len(images)}")

            except Exception as e:
                logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i}: {e}")

        if success_count > 0:
            logger.info(f"âœ… å›¾æ–‡ä¸‹è½½æˆåŠŸ: {success_count}/{len(images)} å¼ ")
            self.stats['success'] += 1
            return True
        else:
            self.stats['failed'] += 1
            return False

    def _download_cover(self, cover_url: str, save_dir: Path, filename: str):
        """ä¸‹è½½å°é¢å›¾ç‰‡"""
        try:
            filepath = save_dir / filename
            if not filepath.exists():
                response = self.session.get(cover_url, timeout=10)
                response.raise_for_status()

                with open(filepath, 'wb') as f:
                    f.write(response.content)

                logger.debug(f"å°é¢ä¸‹è½½æˆåŠŸ: {filename}")
        except Exception as e:
            logger.debug(f"å°é¢ä¸‹è½½å¤±è´¥: {e}")

    def download_from_url(self, url: str, cookies: Dict = None) -> bool:
        """
        ä»URLä¸‹è½½è§†é¢‘

        Args:
            url: è§†é¢‘URL
            cookies: Cookieå­—å…¸

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.stats['total'] += 1

        # è§£æè§†é¢‘ä¿¡æ¯
        video_info = self.parse_video(url, cookies)
        if not video_info:
            logger.error(f"æ— æ³•è§£æè§†é¢‘: {url}")
            self.stats['failed'] += 1
            return False

        # ä¸‹è½½è§†é¢‘
        return self.download_video(video_info)

    def download_from_user(self, user_url: str, cookies: Dict = None, max_videos: int = 10):
        """
        ä¸‹è½½ç”¨æˆ·çš„è§†é¢‘

        Args:
            user_url: ç”¨æˆ·ä¸»é¡µURL
            cookies: Cookieå­—å…¸
            max_videos: æœ€å¤§ä¸‹è½½æ•°é‡
        """
        logger.info(f"ä¸‹è½½ç”¨æˆ·è§†é¢‘: {user_url}")
        logger.warning("ç”¨æˆ·é¡µé¢ä¸‹è½½éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼Œå»ºè®®ä½¿ç”¨å•ä¸ªè§†é¢‘URL")

        # TODO: å®ç°ç”¨æˆ·é¡µé¢è§†é¢‘åˆ—è¡¨è·å–
        # è¿™éœ€è¦è§£æç”¨æˆ·é¡µé¢æˆ–ä½¿ç”¨APIè·å–è§†é¢‘åˆ—è¡¨

    def download_batch(self, urls: List[str], cookies: Dict = None):
        """
        æ‰¹é‡ä¸‹è½½è§†é¢‘

        Args:
            urls: URLåˆ—è¡¨
            cookies: Cookieå­—å…¸
        """
        logger.info(f"æ‰¹é‡ä¸‹è½½: {len(urls)} ä¸ªè§†é¢‘")

        # æ‰¹é‡è§£æ
        results = self.batch_parse(urls, cookies)

        if not results:
            logger.error("æ‰¹é‡è§£æå¤±è´¥")
            return

        # ä¸‹è½½è§†é¢‘
        with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
            futures = []

            for result in results:
                if result.get('success'):
                    video_info = result.get('data')
                    if video_info:
                        future = executor.submit(self.download_video, video_info)
                        futures.append(future)
                else:
                    logger.error(f"è§£æå¤±è´¥: {result.get('url')} - {result.get('error')}")
                    self.stats['failed'] += 1

            # ç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆ
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"ä¸‹è½½å¼‚å¸¸: {e}")

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        # ç§»é™¤éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        # é™åˆ¶é•¿åº¦
        filename = filename[:50]
        # å»é™¤é¦–å°¾ç©ºæ ¼
        filename = filename.strip()
        return filename or "untitled"

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        # è·å–è§£ææœåŠ¡ç»Ÿè®¡
        try:
            response = self.session.get(f"{self.parsing_service_url}/stats")
            if response.status_code == 200:
                service_stats = response.json()
            else:
                service_stats = {}
        except:
            service_stats = {}

        return {
            'downloader': self.stats,
            'parsing_service': service_stats
        }

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ“Š ä¸‹è½½ç»Ÿè®¡")
        print("="*50)
        print(f"æ€»è®¡: {self.stats['total']}")
        print(f"æˆåŠŸ: {self.stats['success']} âœ…")
        print(f"å¤±è´¥: {self.stats['failed']} âŒ")
        print(f"è·³è¿‡: {self.stats['skipped']} â­ï¸")

        if self.stats['total'] > 0:
            success_rate = self.stats['success'] / self.stats['total'] * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='æŠ–éŸ³è§†é¢‘ä¸‹è½½å™¨ V3')
    parser.add_argument('urls', nargs='*', help='è§†é¢‘URLï¼ˆæ”¯æŒå¤šä¸ªï¼‰')
    parser.add_argument('-s', '--service', default='http://localhost:5000',
                       help='è§£ææœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:5000)')
    parser.add_argument('-o', '--output', default='downloads',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: downloads)')
    parser.add_argument('-c', '--cookies', help='Cookieæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-m', '--max-workers', type=int, default=5,
                       help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 5)')
    parser.add_argument('--proxy', action='store_true',
                       help='ä½¿ç”¨ä»£ç†')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶åˆ·æ–°ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰')
    parser.add_argument('-i', '--interactive', action='store_true',
                       help='äº¤äº’æ¨¡å¼')

    args = parser.parse_args()

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = DouYinDownloaderV3(args.service)

    # è®¾ç½®è¾“å‡ºç›®å½•
    downloader.download_dir = Path(args.output)
    downloader.download_dir.mkdir(exist_ok=True)

    # è®¾ç½®é…ç½®
    downloader.config['max_workers'] = args.max_workers
    downloader.config['use_proxy'] = args.proxy
    downloader.config['force_refresh'] = args.force

    # è¯»å–Cookie
    cookies = None
    if args.cookies:
        try:
            cookies = {}
            with open(args.cookies, 'r') as f:
                for line in f:
                    if line.strip() and not line.startswith('#'):
                        parts = line.strip().split('\t')
                        if len(parts) >= 7:
                            cookies[parts[5]] = parts[6]
            logger.info(f"å·²åŠ è½½ {len(cookies)} ä¸ªCookie")
        except Exception as e:
            logger.error(f"è¯»å–Cookieæ–‡ä»¶å¤±è´¥: {e}")

    # äº¤äº’æ¨¡å¼
    if args.interactive or not args.urls:
        print("\n" + "="*50)
        print("ğŸ¬ æŠ–éŸ³è§†é¢‘ä¸‹è½½å™¨ V3 - äº¤äº’æ¨¡å¼")
        print("="*50)
        print("æ”¯æŒçš„URLæ ¼å¼:")
        print("  - çŸ­é“¾æ¥: https://v.douyin.com/xxxxx/")
        print("  - è§†é¢‘é“¾æ¥: https://www.douyin.com/video/xxxxx")
        print("  - ç”¨æˆ·ä¸»é¡µ: https://www.douyin.com/user/xxxxx")
        print("\nè¾“å…¥ 'q' é€€å‡º, 'stats' æŸ¥çœ‹ç»Ÿè®¡")
        print("-"*50)

        while True:
            try:
                url = input("\nè¯·è¾“å…¥URL: ").strip()

                if url.lower() == 'q':
                    break
                elif url.lower() == 'stats':
                    downloader.print_stats()
                    continue
                elif not url:
                    continue

                # æ”¯æŒæ‰¹é‡è¾“å…¥ï¼ˆç©ºæ ¼æˆ–é€—å·åˆ†éš”ï¼‰
                urls = re.split(r'[,\s]+', url)

                if len(urls) == 1:
                    downloader.download_from_url(urls[0], cookies)
                else:
                    downloader.download_batch(urls, cookies)

            except KeyboardInterrupt:
                print("\n\nå·²å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"å¤„ç†é”™è¯¯: {e}")

    # æ‰¹é‡æ¨¡å¼
    elif args.urls:
        if len(args.urls) == 1:
            downloader.download_from_url(args.urls[0], cookies)
        else:
            downloader.download_batch(args.urls, cookies)

    # æ‰“å°ç»Ÿè®¡
    downloader.print_stats()


if __name__ == "__main__":
    main()